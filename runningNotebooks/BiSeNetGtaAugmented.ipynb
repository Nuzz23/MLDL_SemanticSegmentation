{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6oPkLDguraD"
      },
      "source": [
        "## **Configuring the account**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_Ftxi1nuU4I"
      },
      "outputs": [],
      "source": [
        "import subprocess, os\n",
        "# identifying into github\n",
        "!git config --global user.name \"Nuzz23\"\n",
        "!git config --global user.email \"nunzio.licalzi9@gmail.com\"\n",
        "output = subprocess.check_output(\"git config --global --list\", shell=True).decode('utf-8').split()\n",
        "\n",
        "# check if correctly identified\n",
        "assert len(output) >= 2, \"Wrong lenght\"\n",
        "assert output[0].split('=')[-1] == 'Nuzz23', 'wrong user name'\n",
        "assert output[1].split('=')[-1] == 'nunzio.licalzi9@gmail.com', 'wrong email'\n",
        "del output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHcQ0oqAtrmB"
      },
      "outputs": [],
      "source": [
        "with open(\"TOKENS.txt\", 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    os.environ[line.split('=')[0].strip()] = line.strip().split('=')[1].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHfxtL71uuZH"
      },
      "source": [
        "## **Cloning the repository**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ad2J_LONu14E",
        "outputId": "040bb3ef-a201-421a-d399-526164fc4e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "# if the cloned repository already exists delete it\n",
        "%cd /content\n",
        "%rm -rf sample_data\n",
        "if \"MLDL_SemanticSegmentation\" in subprocess.check_output(\"ls\", shell=True).decode(\"utf-8\").strip():\n",
        "  !rm -rf MLDL_SemanticSegmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TxCUei2u4pQ",
        "outputId": "6f6f0bb2-cb0c-4832-ba5e-e54278334e22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MLDL_SemanticSegmentation\n"
          ]
        }
      ],
      "source": [
        "# clone the repository via token\n",
        "!git clone --quiet \"{os.getenv('GITHUB_TOKEN')}\"\n",
        "\n",
        "# check if cloned correctly\n",
        "assert \"MLDL_SemanticSegmentation\" in subprocess.check_output(\"ls\", shell=True).decode(\"utf-8\").strip(), \"Not cloned correctly\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R12eYw0V4pC5"
      },
      "source": [
        "# **LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Vf88t_u64X",
        "outputId": "31195e56-022e-4153-b82a-b8ae04a1cb91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/MLDL_SemanticSegmentation\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# installing the required libraries\n",
        "%cd /content/MLDL_SemanticSegmentation\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VmaXJXN4tP2"
      },
      "source": [
        "## **CUSTOM IMPORTS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz5GC7m50JfP"
      },
      "outputs": [],
      "source": [
        "from stats import countFLOPS, latency, evaluateLastEpoch\n",
        "from train.trainBiSeNetOnGTAAug import init_model\n",
        "from datasets.downloader import Downloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAPefbMJvPAC"
      },
      "source": [
        "# **DATASET**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZsL5E6kvWzR"
      },
      "source": [
        "## **Downloading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfNKyy35vVqF"
      },
      "outputs": [],
      "source": [
        "if not Downloader().downloadCityScapes():\n",
        "  raise FileNotFoundError(\"CityScapes dataset not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBIz9BqI01IJ"
      },
      "outputs": [],
      "source": [
        "if not Downloader().downloadGTA5():\n",
        "  raise FileNotFoundError(\"GTA5 dataset not found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15_bvE6eFo7z"
      },
      "source": [
        "# **MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjU8jiNt6E_y"
      },
      "outputs": [],
      "source": [
        "from datasets.dataAugmentation.horizontalFlip import HorizontalFlip\n",
        "from datasets.dataAugmentation.saltAndPepper import SaltAndPepper\n",
        "from datasets.dataAugmentation.gaussianBlur import GaussianBlur\n",
        "from datasets.dataAugmentation.colorJitter import ColorJitter\n",
        "from datasets.dataAugmentation.deletion import RandomErasing\n",
        "from datasets.dataAugmentation.randomCrop import RandomCrop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "YVdPgoIGn_d3",
        "outputId": "a7bc377d-5884-4f1e-88c4-a2727f58c86d"
      },
      "outputs": [],
      "source": [
        "TRAIN_SIZE, VAL_SIZE = (1280, 720), (1024, 512)\n",
        "\n",
        "model = init_model(model_str='bisenetv2', temperature=0.15, batchSize=4, learning_rate=5e-4, trainSize=TRAIN_SIZE, valSize=VAL_SIZE, momentum=0.9,\n",
        "                   augmentation=[ColorJitter(p=0.50, brightness=0, contrast=0, saturation=0, hue=0.1)],\n",
        "                   enablePrint=True, pushWeights=True, enablePrintVal=True, restartTraining=not True)\n",
        "\n",
        "flops = countFLOPS(model, width=VAL_SIZE[0], height=VAL_SIZE[1])\n",
        "latencyMean, latencyStd, fpsMean, fpsStd = latency(model, width=VAL_SIZE[0], height=VAL_SIZE[1])\n",
        "\n",
        "\n",
        "evaluateLastEpoch(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
